{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchview import draw_graph  # attempts to visualize computational graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from ffmpeg_quality_metrics import FfmpegQualityMetrics as ffqm  # for VMAF calculation\n",
    "from PIL import Image\n",
    "\n",
    "# set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l√§s in data och preprocessa\n",
    "\n",
    "train = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model.eval() is a kind of switch for some specific layers/parts of the model that behave differently \n",
    "during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. \n",
    "You need to turn them off during model evaluation, and .eval() will do it for you.\n",
    "In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation:\n",
    "\"\"\"\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n",
    "model.eval()    # BUT, don't forget to turn back to training mode after eval step: model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
